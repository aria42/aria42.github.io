<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Page Title -->
    <title>Numerical Optimization: Understanding L-BFGS &mdash; aria42</title>
    
      <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,500,600,700|Roboto:400,500,600,700" rel="stylesheet" type="text/css">
    
    <meta name="title" content="Numerical Optimization: Understanding L-BFGS ">
    <!--  Style -->
    <link rel="stylesheet" type="text/css" href="/css/main.css">
    <!-- <link rel="javascript" type="text/javascript" href="/js/main.js"> -->
    <!--  Icon -->
    <link rel="shortcut icon" href="/images/favicon.png" type="image/png" />
    <link rel="apple-touch-icon" href="/images/favicon-ios.png"/>
    <!--  RSS -->
    <link href="/feed.xml" rel="alternate" type="application/rss+xml" title="aria42" />
    <!--  Canonical -->
    <link rel="canonical" href="http://aria42.com/blog/2014/12/understanding-lbfgs">
    <!--  Facebook OG -->
    <meta property="og:title" content="Numerical Optimization: Understanding L-BFGS "/>
    <meta property="og:url" content="http://aria42.com/blog/2014/12/understanding-lbfgs"/>
    
    
      <meta property="og:description" content="Numerical optimization is at the core of much of machine learning. In this post, we derive the L-BFGS algorithm, commonly used in batch machine learning applications."/>
      <meta name="description" content="Numerical optimization is at the core of much of machine learning. In this post, we derive the L-BFGS algorithm, commonly used in batch machine learning applications."/>
    
    <meta property="og:site_name" content="aria42">

    <!-- MathTex -->
    
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.1.1/katex.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.1.1/katex.min.js"></script>
    <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
      </script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        "HTML-CSS": {
          availableFonts: ["TeX"] ,
          width: "container",
          jax: ["input/TeX","output/HTML-SVG"]
        }});
    </script>
    

    <!-- Icon book -->
    <link href="/webfonts/ss-social-circle.css" rel="stylesheet" />
</head>
<body>

<section class="site-nav">
    <header>
        <nav id="navigation">
            <ul id="navigation-menu">
                <li>
                  <a class="logo nav-button-home" href="/">
                    <?xml version="1.0" encoding="UTF-8" standalone="no"?>
<svg width="172px" height="173px" viewBox="0 0 172 173" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:sketch="http://www.bohemiancoding.com/sketch/ns">
    <!-- Generator: Sketch 3.1.1 (8761) - http://www.bohemiancoding.com/sketch -->
    <title>logo</title>
    <desc>Created with Sketch.</desc>
    <defs></defs>
    <g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd" sketch:type="MSPage">
        <g id="logo" sketch:type="MSLayerGroup" transform="translate(-12.000000, -9.659028)">
            <ellipse id="Oval-1" fill="#444444" sketch:type="MSShapeGroup" cx="97.9918197" cy="96.5029479" rx="85.9918197" ry="86.0129479"></ellipse>
            <text id="42" sketch:type="MSTextLayer" font-family="Plantagenet Cherokee" font-size="179" font-weight="normal" sketch:alignment="middle" fill="#FFFFFF">
                <tspan x="17.777" y="125">42</tspan>
            </text>
        </g>
    </g>
</svg>
                  </a>
                </li>
            
                <li>
                  <a class="nav-text-button nav-button-blog" href="/blog">
                      BLOG
                  </a>
                </li>
            
                <li>
                  <a class="nav-text-button nav-button-academic" href="/academic">
                      ACADEMIC
                  </a>
                </li>
            
                <li>
                  <a class="nav-text-button nav-button-media" href="/media">
                      MEDIA
                  </a>
                </li>
            
            </ul>
        </nav>
    </header>
</section>


<article>

    <div class="container">
        <header>
            <div class="meta">
                <time pubdate datetime="2014-02-December" title="December 02, 2014">December 02, 2014</time>
            </div>
            <h1 class="title"><a href="/blog/2014/12/understanding-lbfgs">Numerical Optimization: Understanding L-BFGS</a></h1>
            
        </header>

        <section>
            <div style="display:none">
$$
\newcommand{\hessian}{\mathbf{H}}
\newcommand{\grad}{\mathbf{g}}
\newcommand{\invhessian}{\mathbf{H}^{-1}}
\newcommand{\qquad}{\hspace{1em}}
$$
</div>

<p>Numerical optimization is at the core of much of machine learning. Once you’ve defined your model and have a dataset ready, estimating the parameters of your model typically boils down to minimizing some <a href="http://en.wikipedia.org/wiki/Multivariable_calculus">multivariate function</a> $f(x)$, where the input $x$ is in some high-dimensional space and corresponds to model parameters. In other words, if you solve:</p>

<script type="math/tex; mode=display">x^* = \arg\min_x f(x)</script>

<p>then $x^*$ is the ‘best’ choice for model parameters according to how you’ve set your objective.<sup id="fnref:global-min"><a href="#fn:global-min" class="footnote">1</a></sup></p>

<p>In this post, I’ll focus on the motivation for the <a href="http://en.wikipedia.org/wiki/Limited-memory_BFGS">L-BFGS</a> algorithm for unconstrained function minimization, which is very popular for ML problems where ‘batch’  optimization makes sense. For larger problems, online methods based around <a href="http://en.wikipedia.org/wiki/Stochastic_gradient_descent">stochastic gradient descent</a> have gained popularity, since they require fewer passes over data to converge. In a later post, I might cover some of these techniques, including my personal favorite <a href="http://www.matthewzeiler.com/pubs/googleTR2012/googleTR2012.pdf">AdaDelta</a>.</p>

<p><strong>Note</strong>: Throughout the post, I’ll assume you remember multivariable calculus. So if you don’t recall what a <a href="http://en.wikipedia.org/wiki/Gradient">gradient</a> or <a href="http://en.wikipedia.org/wiki/Hessian_matrix">Hessian</a> is, you’ll want to bone up first.</p>

<p><img src="/images/steepest-descent.png" alt="&quot;Illustration of iterative function descent&quot;" title="Illustration of iterative function descent" /></p>

<h1 id="newtons-method">Newton’s Method</h1>

<p>Most numerical optimization procedures are iterative algorithms which consider a sequence of ‘guesses’ $x_n$ which ultimately converge to $x^*$ the true global minimizer of $f$. Suppose, we have an estimate $x_n$ and we want our next estimate $x_{n+1}$ to have the property that <script type="math/tex">% <![CDATA[
f(x_{n+1}) < f(x_n) %]]></script>.</p>

<p>Newton’s method is centered around a quadratic approximation of $f$ for points near $x_n$.
Assuming that $f$ is twice-differentiable, we can use a quadratic approximation of $f$ for points ‘near’ a fixed point $x$ using a <a href="http://en.wikipedia.org/wiki/Taylor_series">Taylor expansion</a>:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
f(x + \Delta x)
&\approx f(x) + \Delta x^T \nabla f(x)  + \frac{1}{2} \Delta x^T \left( \nabla^2 f(x) \right)  \Delta x
\end{align} %]]></script>

<p>where $\nabla f(x)$ and $\nabla^2 f(x)$ are the gradient and Hessian of $f$ at the point $x_n$. This approximation holds in the limit as $|| \Delta x || \rightarrow 0$. This is a generalization of the single-dimensional Taylor polynomial expansion you might remember from Calculus.</p>

<p>In order to simplify much of the notation, we’re going to think of our iterative algorithm of producing a sequence of such quadratic approximations $h_n$. Without loss of generality, we can write <script type="math/tex">x_{n+1} = x_n + \Delta x</script> and re-write the above equation,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
h_n(\Delta x) &= f(x_n) + \Delta x^T \grad_n + \frac{1}{2} \Delta x^T \hessian_n  \Delta x
\end{align} %]]></script>

<p>where $\grad_n$ and $\hessian_n$ represent the gradient and Hessian of $f$ at $x_n$.</p>

<p>We want to choose $\Delta x$ to minimize this local quadratic approximation of $f$ at $x_n$. Differentiating with respect to $\Delta x$ above yields:</p>

<script type="math/tex; mode=display">\begin{align}
\frac{\partial h_n(\Delta x)}{\partial \Delta x} = \grad_n + \hessian_n \Delta x
\end{align}</script>

<p>Recall that any $\Delta x$ which yields $\frac{\partial h_n(\Delta x)}{\partial \Delta x} = 0$ is a local extrema of $h_n(\cdot)$. If we assume that $\hessian_n$ is [postive definite] (psd) then we know this $\Delta x$ is also a global minimum for $h_n(\cdot)$. Solving for $\Delta x$:<sup id="fnref:why-global"><a href="#fn:why-global" class="footnote">2</a></sup></p>

<script type="math/tex; mode=display">\Delta x = - \invhessian_n \grad_n</script>

<p>This suggests $\invhessian_n \grad_n$ as a good direction to move $x_n$ towards. In practice, we set <script type="math/tex">x_{n+1} = x_n - \alpha (\invhessian_n \grad_n)</script> for a value of $\alpha$ where $f(x_{n+1})$ is  ‘sufficiently’ smaller than $f(x_n)$.</p>

<h2 id="iterative-algorithm">Iterative Algorithm</h2>

<p>The above suggests an iterative algorithm:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
 & \mathbf{NewtonRaphson}(f,x_0): \\
 & \qquad \mbox{For $n=0,1,\ldots$ (until converged)}: \\
 & \qquad \qquad \mbox{Compute $\grad_n$ and $\invhessian_n$ for $x_n$} \\
 & \qquad \qquad d = \invhessian_n \grad_n \\
 & \qquad \qquad \alpha = \min_{\alpha \geq 0} f(x_{n} - \alpha d) \\
 & \qquad \qquad x_{n+1} \leftarrow x_{n} - \alpha d
\end{align} %]]></script>

<p>The computation of the $\alpha$ step-size can use any number of <a href="http://en.wikipedia.org/wiki/Line_search">line search</a> algorithms. The simplest of these is <a href="http://en.wikipedia.org/wiki/Backtracking_line_search">backtracking line search</a>, where you simply try smaller and smaller values of $\alpha$ until the function value is ‘small enough’.</p>

<p>In terms of software engineering, we can treat $\mathbf{NewtonRaphson}$ as a blackbox for any twice-differentiable function which satisfies the Java interface:</p>

<figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">interface</span> <span class="nc">TwiceDifferentiableFunction</span> <span class="o">{</span>
  <span class="c1">// compute f(x)
</span>
  <span class="kd">public</span> <span class="kt">double</span> <span class="n">valueAt</span><span class="o">(</span><span class="kt">double</span><span class="o">[]</span> <span class="n">x</span><span class="o">);</span>

  <span class="c1">// compute grad f(x)
</span>
  <span class="kd">public</span> <span class="kt">double</span><span class="o">[]</span> <span class="n">gradientAt</span><span class="o">(</span><span class="kt">double</span><span class="o">[]</span> <span class="n">x</span><span class="o">);</span>

  <span class="c1">// compute inverse hessian H^-1
</span>
  <span class="kd">public</span> <span class="kt">double</span><span class="o">[][]</span> <span class="n">inverseHessian</span><span class="o">(</span><span class="kt">double</span><span class="o">[]</span> <span class="n">x</span><span class="o">);</span>
<span class="o">}</span></code></pre></figure>

<p>With quite a bit of tedious math, you can prove that for a <a href="http://en.wikipedia.org/wiki/Convex_function">convex function</a>, the above procedure will converge to a unique global minimizer $x^*$, regardless of the choice of $x_0$. For non-convex functions that arise in ML (almost all latent variable models or deep nets), the procedure still works but is only guranteed to converge to a local minimum. In practice, for non-convex optimization, users need to pay more attention to initialization and other algorithm details.</p>

<h2 id="huge-hessians">Huge Hessians</h2>

<p>The central issue with $\mathbf{NewtonRaphson}$ is that we need to be able to compute the inverse Hessian matrix.<sup id="fnref:implicit-multiply"><a href="#fn:implicit-multiply" class="footnote">3</a></sup> Note that for ML applications, the dimensionality of the input to $f$ typically corresponds to model parameters. It’s not unusual to have hundreds of millions of parameters or in some vision applications even <a href="http://static.googleusercontent.com/media/research.google.com/en/us/archive/large_deep_networks_nips2012.pdf">billions of parameters</a>. For these reasons, computing the hessian or its inverse is often impractical. For many functions, the hessian may not even be analytically computable, let along representable.</p>

<p>Because of these reasons, $\mathbf{NewtonRaphson}$ is rarely used in practice to optimize functions corresponding to large problems. Luckily, the above algorithm can still work even if $\invhessian_n$ doesn’t correspond to the exact inverse hessian at $x_n$, but is instead a good approximation.</p>

<h1 id="quasi-newton">Quasi-Newton</h1>

<p>Suppose that instead of requiring $\invhessian_n$ be the inverse hessian at $x_n$, we think of it as an approximation of this information. We can generalize $\mathbf{NewtonRaphson}$ to take a $\mbox{QuasiUpdate}$ policy which is responsible for producing a sequence of $\invhessian_n$.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
& \mathbf{QuasiNewton}(f,x_0, \invhessian_0, \mbox{QuasiUpdate}): \\
& \qquad \mbox{For $n=0,1,\ldots$ (until converged)}: \\
& \qquad \qquad \mbox{// Compute search direction and step-size } \\
& \qquad \qquad d = \invhessian_n \grad_n \\
& \qquad \qquad \alpha \leftarrow \min_{\alpha \geq 0} f(x_{n} - \alpha d) \\
& \qquad \qquad x_{n+1} \leftarrow x_{n} - \alpha d \\
& \qquad \qquad \mbox{// Store the input and gradient deltas } \\
& \qquad \qquad \grad_{n+1} \leftarrow \nabla f(x_{n+1}) \\
& \qquad \qquad s_{n+1} \leftarrow x_{n+1} - x_n \\
& \qquad \qquad y_{n+1} \leftarrow \grad_{n+1} - \grad_n \\
& \qquad \qquad \mbox{// Update inverse hessian } \\
& \qquad \qquad \invhessian_{n+1} \leftarrow \mbox{QuasiUpdate}(\invhessian_{n},s_{n+1}, y_{n+1})
\end{align} %]]></script>

<p>We’ve assumed that $\mbox{QuasiUpdate}$ only requires the former inverse hessian estimate as well tas the input and gradient differences ($s_n$ and $y_n$ respectively). Note that if $\mbox{QuasiUpdate}$ just returns $\nabla^2 f(x_{n+1})$, we recover exact $\mbox{NewtonRaphson}$.</p>

<p>In terms of software, we can blackbox optimize an arbitrary differentiable function (with no need to be able to compute a second derivative) using $\mathbf{QuasiNewton}$ assuming we get a quasi-newton approximation update policy. In Java this might look like this,</p>

<figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">interface</span> <span class="nc">DifferentiableFunction</span> <span class="o">{</span>
  <span class="c1">// compute f(x)
</span>
  <span class="kd">public</span> <span class="kt">double</span> <span class="n">valueAt</span><span class="o">(</span><span class="kt">double</span><span class="o">[]</span> <span class="n">x</span><span class="o">);</span>

  <span class="c1">// compute grad f(x)
</span>
  <span class="kd">public</span> <span class="kt">double</span><span class="o">[]</span> <span class="n">gradientAt</span><span class="o">(</span><span class="kt">double</span><span class="o">[]</span> <span class="n">x</span><span class="o">);</span>  
<span class="o">}</span>

<span class="kd">public</span> <span class="kd">interface</span> <span class="nc">QuasiNewtonApproximation</span> <span class="o">{</span>
  <span class="c1">// update the H^{-1} estimate (using x_{n+1}-x_n and grad_{n+1}-grad_n)
</span>
  <span class="kd">public</span> <span class="kt">void</span> <span class="n">update</span><span class="o">(</span><span class="kt">double</span><span class="o">[]</span> <span class="n">deltaX</span><span class="o">,</span> <span class="kt">double</span><span class="o">[]</span> <span class="n">deltaGrad</span><span class="o">);</span>

  <span class="c1">// H^{-1} (direction) using the current H^{-1} estimate
</span>
  <span class="kd">public</span> <span class="kt">double</span><span class="o">[]</span> <span class="n">inverseHessianMultiply</span><span class="o">(</span><span class="kt">double</span><span class="o">[]</span> <span class="n">direction</span><span class="o">);</span>
<span class="o">}</span></code></pre></figure>

<p>Note that the only use we have of the hessian is via it’s product with the gradient direction. This will become useful for the L-BFGS algorithm described below, since we don’t need to represent the Hessian approximation in memory. If you want to see these abstractions in action, here’s a link to a <a href="https://github.com/aria42/java8-optimize/tree/master/src/optimize">Java 8</a> and  <a href="https://github.com/aria42/taskar/blob/master/optimize/newton.go">golang</a> implementation I’ve written.</p>

<h2 id="behave-like-a-hessian">Behave like a Hessian</h2>

<p>What form should $\mbox{QuasiUpdate}$ take? Well, if we have $\mbox{QuasiUpdate}$ always return the identity matrix (ignoring its inputs), then this corresponds to simple <a href="http://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a>, since the search direction is always $\nabla f_n$. While this actually yields a valid procedure which will converge to $x^*$ for convex $f$, intuitively this choice of $\mbox{QuasiUpdate}$ isn’t attempting to capture second-order information about $f$.</p>

<p>Let’s think about our choice of <script type="math/tex">\hessian_{n}</script> as an approximation for $f$ near $x_{n}$:</p>

<script type="math/tex; mode=display">h_{n}(d) = f(x_{n}) + d^T \grad_{n} + \frac{1}{2} d^T \hessian_{n} d</script>

<h3 id="secant-condition">Secant Condition</h3>

<p>A good property for <script type="math/tex">h_{n}(d)</script> is that its gradient agrees with $f$ at $x_n$ and $x_{n-1}$. In other words, we’d like to ensure:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\nabla h_{n}(x_{n}) &= \grad_{n} \\
\nabla h_{n}(x_{n-1}) &= \grad_{n-1}\\
\end{align} %]]></script>

<p>Using both of the equations above:</p>

<script type="math/tex; mode=display">\nabla h_{n}(x_{n}) - \nabla h_{n}(x_{n-1}) = \grad_{n} - \grad_{n-1}</script>

<p>Using the gradient of $h_{n+1}(\cdot)$ and canceling terms we get</p>

<script type="math/tex; mode=display">\hessian_{n}(x_{n} - x_{n-1}) = (\grad_{n} - \grad_{n-1}) \\</script>

<p>This yields the so-called “secant conditions” which ensures that $\hessian_{n+1}$ behaves like the Hessian at least for the diference <script type="math/tex">(x_{n} - x_{n-1})</script>. Assuming <script type="math/tex">\hessian_{n}</script> is invertible (which is true if it is psd), then multiplying both sides by <script type="math/tex">\invhessian_{n}</script> yields</p>

<script type="math/tex; mode=display">\invhessian_{n} \mathbf{y}_{n}   = \mathbf{s}_{n}</script>

<p>where <script type="math/tex">\mathbf{y}_{n+1}</script> is the difference in gradients and <script type="math/tex">\mathbf{s}_{n+1}</script> is the difference in inputs.</p>

<h3 id="symmetric">Symmetric</h3>

<p>Recall that the a hessian represents the matrix of 2nd order partial derivatives: $\hessian^{(i,j)} = \partial f / \partial x_i \partial x_j$. The hessian is symmetric since the order of differentiation doesn’t matter.</p>

<h3 id="the-bfgs-update">The BFGS Update</h3>

<p>Intuitively, we want $\hessian_n$ to satisfy the two conditions above:</p>

<ul>
  <li>Secant condition holds for $\mathbf{s}_n$ and $\mathbf{y}_n$</li>
  <li>$\hessian_n$ is symmetric</li>
</ul>

<p>Given the two conditions above, we’d like to take the most conservative change relative to $\hessian_{n-1}$. This is reminiscent of the <a href="http://aria42.com/blog/2010/09/classification-with-mira-in-clojure/">MIRA update</a>, where we have conditions on any good solution but all other things equal, want the ‘smallest’ change.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\min_{\invhessian} & \hspace{0.5em} \| \invhessian - \invhessian_{n-1} \|^2 \\
\mbox{s.t. } & \hspace{0.5em} \invhessian \mathbf{y}_{n}   = \mathbf{s}_{n} \\
            & \hspace{0.5em} \invhessian \mbox{ is symmetric }
\end{aligned} %]]></script>

<p>The norm used here <script type="math/tex">\| \cdot \|</script> is the <a href="http://mathworld.wolfram.com/FrobeniusNorm.html">weighted frobenius norm</a>.<sup id="fnref:weighted-norm"><a href="#fn:weighted-norm" class="footnote">4</a></sup> The solution to this optimization problem is given by</p>

<script type="math/tex; mode=display">\invhessian_{n+1} = (I - \rho_n y_n s_n^T) \invhessian_n (I - \rho_n s_n y_n^T) + \rho_n s_n s_n^T</script>

<p>where $\rho_n = (y_n^T s_n)^{-1}$. Proving this is relatively involved and mostly symbol crunching. I don’t know of any intuitive way to derive this unfortunately.</p>

<p><img src="/images/bfgs.png" /></p>

<p>This update is known as the Broyden–Fletcher–Goldfarb–Shanno (BFGS) update, named after the original authors. Some things worth noting about this update:</p>

<ul>
  <li>
    <p>$\invhessian_{n+1}$ is positive definite (psd) when $\invhessian_n$ is. Assuming our initial guess of  $\hessian_0$ is psd, it follows by induction each inverse Hessian estimate is as well. Since we can choose any $\invhessian_0$ we want, including the $I$ matrix, this is easy to ensure.</p>
  </li>
  <li>
    <p>The above also specifies a recurrence relationship between <script type="math/tex">\invhessian_{n+1}</script> and <script type="math/tex">\invhessian_{n}</script>. We only need the history of <script type="math/tex">s_n</script> and <script type="math/tex">y_n</script> to re-construct <script type="math/tex">\invhessian_n</script>.</p>
  </li>
</ul>

<p>The last point is significant since it will yield a procedural algorithm for computing $\invhessian_n d$, for a direction $d$, without ever forming the $\invhessian_n$ matrix. Repeatedly applying the recurrence above we have</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
& \mathbf{BFGSMultiply}(\invhessian_0, \{s_k\}, \{y_k\}, d): \\
& \qquad r \leftarrow d \\
& \qquad \mbox{// Compute right product} \\
& \qquad \mbox{for $i=n,\ldots,1$}: \\
& \qquad \qquad \alpha_i \leftarrow \rho_{i} s^T_i r \\
& \qquad \qquad r \leftarrow r - \alpha_i y_i \\
& \qquad \mbox{// Compute center} \\
& \qquad r \leftarrow \invhessian_0 r \\
& \qquad \mbox{// Compute left product} \\
& \qquad \mbox{for $i=1,\ldots,n$}: \\
& \qquad \qquad \beta \leftarrow \rho_{i} y^T_i r \\
& \qquad \qquad r \leftarrow r + (\alpha_{n-i+1}-\beta)s_i \\
& \qquad \mbox{return $r$}
\end{align} %]]></script>

<p>Since the only use for $\invhessian_n$ is via the product $\invhessian_n \grad_n$, we only need the above procedure to use the BFGS approximation in $\mbox{QuasiNewton}$.</p>

<h3 id="l-bfgs-bfgs-on-a-memory-budget">L-BFGS: BFGS on a memory budget</h3>

<p>The BFGS quasi-newton approximation has the benefit of not requiring us to be able to analytically compute the Hessian of a function. However, we still must maintain a history of the $s_n$ and $y_n$ vectors for each iteration. Since one of the core-concerns of the $\mathbf{NewtonRaphson}$ algorithm were the memory requirements associated with maintaining an Hessian, the BFGS Quasi-Newton algorithm doesn’t address that since our memory use can grow without bound.</p>

<p>The L-BFGS algorithm, named for <em>limited</em> BFGS, simply truncates the <script type="math/tex">\mathbf{BFGSMultiply}</script> update to use the last $m$ input differences and gradient differences. This means, we only need to store <script type="math/tex">s_n, s_{n-1},\ldots, s_{n-m-1}</script> and <script type="math/tex">y_n, y_{n-1},\ldots, y_{n-m-1}</script> to compute the update. The center product can still use any symmetric psd matrix <script type="math/tex">\invhessian_0</script>, which can also depend on any <script type="math/tex">\{s_k\}</script> or <script type="math/tex">\{ y_k \}</script>.</p>

<h1 id="l-bfgs-variants">L-BFGS variants</h1>

<p>There are lots of variants of L-BFGS which get used in practice. For non-differentiable functions, there is an <a href="http://research.microsoft.com/en-us/um/people/jfgao/paper/icml07scalable.pdf">othant-wise varient</a> which is suitable for training $L_1$ regularized loss.</p>

<p>One of the main reasons to <em>not</em> use L-BFGS is in very large data-settings where an online approach can converge faster. There are in fact <a href="http://jmlr.org/proceedings/papers/v2/schraudolph07a/schraudolph07a.pdf">online variants</a> of L-BFGS, but to my knowledge, none have consistently out-performed SGD variants (including <a href="http://www.magicbroom.info/Papers/DuchiHaSi10.pdf">AdaGrad</a> or AdaDelta) for sufficiently large data sets.</p>

<!-- Footnotes and Links -->

<div class="footnotes">
  <ol>
    <li id="fn:global-min">
      <p>This assumes there is a unique global minimizer for $f$. In practice, in practice unless $f$ is convex, the parameters used are whatever pops out the other side of an iterative algorithm. <a href="#fnref:global-min" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:why-global">
      <p>We know $- \invhessian \nabla f$ is a local extrema since the gradient is zero, since the Hessian has positive curvature, we know it’s in fact a local minima. If $f$ is convex, we know the Hessian is always positive definite and we know there is a single unique global minimum. <a href="#fnref:why-global" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:implicit-multiply">
      <p>As we’ll see, we really on require being able to multiply by $\invhessian d$ for a direction $d$. <a href="#fnref:implicit-multiply" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:weighted-norm">
      <p>I’ve intentionally left the weighting matrix $W$ used to weight the norm since you get the same solution under many choices. In particular for any positive-definite $W$ such that $W s_n = y_n$, we get the same solution. <a href="#fnref:weighted-norm" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

        </section>
    </div>
</article>


<footer class="site-footer">
        <div class="contact">
            <a href="http://twitter.com/aria42" class="home"><i class="ss-icon">twitter</i></a>
            <a href="http://github.com/aria42" class="home"><i class="ss-icon">octocat</i></a>
            <a href="http://linkedin.com/in/aria42" class="home"><i class="ss-icon">linkedin</i></a>
            <a href="mailto:me@aria42.com" class="home" style="margin-right:0em"><i class="ss-icon">email</i></a>
        </div>
        <div class="blah">
        &copy; 2016 <a href="http://aria42.com/">aria42.com</a>
        </div>
</footer>

<script type="text/javascript" src="//ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js"></script>





<script src="/webfonts/ss-social.js"></script>
<script>
  function toggleAbstract(elem) {
      $(elem).parents(".academic-paper").find(".abstract").toggle();
  }

  $(".full img").on("click", function() {
  $(this).toggleClass("zoom");
});

SVGElement.prototype.addClass = function (className) {
  if (!this.hasClass(className)) {
    this.setAttribute('class', this.getAttribute('class') + ' ' + className);
  }
};

$("a.nav-text-button").on({ 'touchstart' : function(){
  $(this).addClass('active');
}});

$("a.nav-text-button").on({ 'touchend' : function(){
  $(this).removeClass('active');
}});
$("a.nav-text-button").on({ 'touchleave' : function(){
  $(this).removeClass('active');
}});

function svgFill() {
  $('img[src$="svg"]').hide()
    .each(function(i, item) {
      var _this = this;
      return $.get(this.src).success(function(data) {
        var $svg, a, nName, nValue, _attr, _i, _len;
        $svg = $(data).find('svg');
        _attr = _this.attributes;
        $.extend(_attr, $svg[0].attributes);
        for (_i = 0, _len = _attr.length; _i < _len; _i++) {
          a = _attr[_i];
          nName = a.nodeName;
          nValue = a.nodeValue;
          if (nName !== 'src' && nName !== 'style') {
            $svg.attr(nName, nValue);
          }
        }
        return $(_this).replaceWith($svg);
      });
  });
}

var accented = "#428bca";

function setupHeader() {
  var uri = window.location.pathname.substring(1);
  if (uri.indexOf("/") >= 0) {
    uri = uri.substring(0, uri.indexOf("/"));
  }
  if (uri == "") {
    var loop = setInterval(function() {
      if ($(".nav-button-home svg ellipse").length) {
        $(".nav-button-home svg ellipse").attr("fill",accented);
      }
    }, 5)
  } else {
    $(".nav-button-" + uri).addClass("accented");
  }
}

(function() {
  $(svgFill);
  $(setupHeader);
}).call(this);


</script>

  <!--google analytics tracking code here-->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-57015155-1', 'auto');
  ga('send', 'pageview');

</script>
<!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=5151559;
var sc_invisible=1;
var sc_security="c3abe7d5";
</script>
<script type="text/javascript"
src="http://www.statcounter.com/counter/counter.js"></script>
<!-- End of StatCounter Code for Default Guide -->




</body>
</html>
