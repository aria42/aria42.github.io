<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Page Title -->
    <title>Clojure Unsupervised Part-Of-Speech Tagger Explained &mdash; aria42</title>
    
      <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,500,600,700|Roboto:400,500,600,700" rel="stylesheet" type="text/css">
    
    <meta name="title" content="Clojure Unsupervised Part-Of-Speech Tagger Explained ">
    <!--  Style -->
    <link rel="stylesheet" type="text/css" href="/css/main.css">
    <!-- <link rel="javascript" type="text/javascript" href="/js/main.js"> -->
    <!--  Icon -->
    <link rel="shortcut icon" href="/images/favicon.png" type="image/png" />
    <link rel="apple-touch-icon" href="/images/favicon-ios.png"/>
    <!--  RSS -->
    <link href="/feed.xml" rel="alternate" type="application/rss+xml" title="aria42" />
    <!--  Canonical -->
    <link rel="canonical" href="http://aria42.com/blog/2010/09/clojure-unsupervised-part-of-speech-tagger-explained">
    <!--  Facebook OG -->
    <meta property="og:title" content="Clojure Unsupervised Part-Of-Speech Tagger Explained "/>
    <meta property="og:url" content="http://aria42.com/blog/2010/09/clojure-unsupervised-part-of-speech-tagger-explained"/>
    
    
      <meta property="og:description" content="Analaysis of the Clojure code sample that imprlements unsupervised part-of-speech tagger."/>
      <meta name="description" content="Analaysis of the Clojure code sample that imprlements unsupervised part-of-speech tagger."/>
    
    <meta property="og:site_name" content="aria42">

    <!-- MathTex -->
    
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.1.1/katex.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.1.1/katex.min.js"></script>
    <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
      </script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        "HTML-CSS": {
          availableFonts: ["TeX"] ,
          width: "container",
          jax: ["input/TeX","output/HTML-SVG"]
        }});
    </script>
    

    <!-- Icon book -->
    <link href="/webfonts/ss-social-circle.css" rel="stylesheet" />
</head>
<body>

<section class="site-nav">
    <header>
        <nav id="navigation">
            <ul id="navigation-menu">
                <li>
                  <a class="logo nav-button-home" href="/">
                    <?xml version="1.0" encoding="UTF-8" standalone="no"?>
<svg width="172px" height="173px" viewBox="0 0 172 173" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:sketch="http://www.bohemiancoding.com/sketch/ns">
    <!-- Generator: Sketch 3.1.1 (8761) - http://www.bohemiancoding.com/sketch -->
    <title>logo</title>
    <desc>Created with Sketch.</desc>
    <defs></defs>
    <g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd" sketch:type="MSPage">
        <g id="logo" sketch:type="MSLayerGroup" transform="translate(-12.000000, -9.659028)">
            <ellipse id="Oval-1" fill="#444444" sketch:type="MSShapeGroup" cx="97.9918197" cy="96.5029479" rx="85.9918197" ry="86.0129479"></ellipse>
            <text id="42" sketch:type="MSTextLayer" font-family="Plantagenet Cherokee" font-size="179" font-weight="normal" sketch:alignment="middle" fill="#FFFFFF">
                <tspan x="17.777" y="125">42</tspan>
            </text>
        </g>
    </g>
</svg>
                  </a>
                </li>
            
                <li>
                  <a class="nav-text-button nav-button-blog" href="/blog">
                      BLOG
                  </a>
                </li>
            
                <li>
                  <a class="nav-text-button nav-button-academic" href="/academic">
                      ACADEMIC
                  </a>
                </li>
            
                <li>
                  <a class="nav-text-button nav-button-media" href="/media">
                      MEDIA
                  </a>
                </li>
            
            </ul>
        </nav>
    </header>
</section>


<article>

    <div class="container">
        <header>
            <div class="meta">
                <time pubdate datetime="2010-19-September" title="September 19, 2010">September 19, 2010</time>
            </div>
            <h1 class="title"><a href="/blog/2010/09/clojure-unsupervised-part-of-speech-tagger-explained">Clojure Unsupervised Part-Of-Speech Tagger Explained</a></h1>
            
        </header>

        <section>
            <p><a href="http://aria42.com/blog/?p=33">Last week</a>, I posted a <a href="http://gist.github.com/578348">300 line clojure script</a> which implements some <a href="http://www.cs.berkeley.edu/~aria42/pubs/typetagging.pdf">recent work</a> I’ve published in <a href="http://en.wikipedia.org/wiki/Part-of-speech_tagging">unsupervised part-of-speech tagging</a>. In this post, I’m going to describe more fully how the model works and also how the implementation works. This post is going to assume that you have some basic background in probability and that you know some clojure. The post is massive, so feel free to skip sections if you feel like something is too remedial; I’ve  put superfluous details in footnotes or marked paragraphs.</p>

<h2 id="what-exactly-is-unsupervised-part-of-speech-tagging">What exactly is unsupervised part-of-speech tagging?</h2>

<p>Unsupervised <a href="http://en.wikipedia.org/wiki/Part-of-speech_tagging">part-of-speech (POS) tagging</a> is the task of taking the raw text of a language and inducing syntactic categories of words. For instance, in English, the words “dog”,”basketball”, and “compiler”, aren’t semantically related, but all are common nouns. You probably know the basic syntactic categories of words: nouns, verbs, adjectives, adverbs, etc. However, natural language processing (NLP) applications typically require more fine grained distinctions. For instance, the difference between a singular, plural, or proper nouns. In English, the most commonly used annotated POS data has 45 different tags.</p>

<p>What we’re interested in here is <a href="http://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised learning</a>, meaning that at no point does the model get told about the kinds of syntactic categories we want nor does it get examples of annotated sentences or examples (there is no <emph>supervised</emph> data); you just get raw data. There are several advantages to using unsupervised learning, not least of which being there are languages that don’t have POS annotated data.</p>

<p>A subtle consequence of being unsupervised is that we aren’t going to directly learn that the word “dog” is a singular common noun. Instead, we learn there are some fixed number of tag states and all the things we call a singular common noun may map to tag state 42, for instance. Basically, the tags in the model don’t come with the names we recognize, we have to map them to meaningful names (if that’s what you’re application requires). Essentially, what you get out of this is a clustering over words which corresponds to meaningful syntactic distinctions.</p>

<h3 id="how-does-the-model-work">How does the model work?</h3>
<p>The model is a variation on the standard Hidden Markov Model (HMM), which I’ll briefly recap. The HMM unsupervised POS tagging story works as follows: We assume a fixed number of possible tag states, <script type="math/tex">K</script> as well as a fixed vocabulary of size  <script type="math/tex">V</script>. The Markov model part of HMM refers to the fact that the probability distribution of tag states for a single sentence is generated under a first-order Markov assumption. I.e., the probability
$P(t_1,\ldots,t_m)$ for a sentence of length $m$ is given by,</p>

<script type="math/tex; mode=display">P(t_1,\ldots,t_m) = \prod_{i=1}^m P(t_{i+1} | t_i)</script>

<p>This encodes the intuition that typically some kind of noun or adjectives usually follows a determiner (e.g. “the”,”a”,”this”).<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup></p>

<div>
Once the tags of the sentence have been generated, for each position, a word is drawn conditioned on the underlying tag. Specifically, for $i=1,\ldots,n$ a word $w_i$ is drawn conditioned on the corresponding tag  $t_i$, $P(w_i | t_i)$. This <emph>emission</emph> distribution is parametrized according to parameters $\theta_t$ for each tag $t$ over the $V$ vocabulary elements. So for instance, for tag state 42, which we suppose corresponds to a singular noun, there is a distribution over all possible singular nouns, that might look like this:<sup id="fnref:42"><a href="#fn:42" class="footnote">1</a></sup>
</div>

<script type="math/tex; mode=display">\theta_{42} =  \left\{ dog: 0.03, basketball: 0.02, compiler: 0.01, \ldots \right\}</script>

<p>If we let, <script type="math/tex">\mathbf{w}</script> denote a corpus, consisting of a bunch of sentences, then the HMM puts mass on the corpus as well as corresponding
tag sequences <script type="math/tex">\mathbf{t}</script> as follows,</p>

<script type="math/tex; mode=display">P(\mathbf{w},\mathbf{t}) = \prod_{(w,t) \in (\mathbf{w},\mathbf{t})} P(w,t) = \prod_{(w,t) \in (\mathbf{w},\mathbf{t})} \left( \prod_{i=0}^m P(w_{i+1} | t_{i+1}) P(t_{i+1} | t_i) \right)</script>

<h3>What's wrong with the HMM?</h3>
<p>There’s a lot wrong with the HMM approach to learning POS structure. One of the most important is that the model doesn’t encode the constraint that a given word typically should be associated with a few number of tags. The model is perfectly happy to learn that a given word can be generated any number of tags. A lot of doing unsupervised machine learning is understanding how to alter models to reflect the constraints and preferences that the structure we are interested in has.</p>

<p>Another more subtle issue is that there is a significant skew to the number of words which belong to each part-of-speech category. For instance, there are very few determiners in most languages, but they appear very frequently at the token level. There is no way to encode this constraint that some tags are infrequent or frequent at the <emph>type-level</emph> (have very few (or many) unique word types that can use a given tag category). So the model has a prior <script type="math/tex">P(T)</script> over tag assignments to words.<sup id="fnref:distribution"><a href="#fn:distribution" class="footnote">2</a></sup></p>

<h3>What's the approach in the paper?</h3>
<p>The approach in the paper is actually very simple: For each word type <script type="math/tex">W_i</script>, assign it a single legal tag state <script type="math/tex">T_i</script>. So for the word type “dog”, the model chooses a single legal tag (amongst <script type="math/tex">t=1,\ldots,K</script>); essentially, decisions are made for each word once at the  type level, rather than at the token-level for each individual instantiation of the word. Once this has been done for the entire vocabulary, these type tag assignments constrain the HMM <script type="math/tex">\theta_t</script> parameters so only words assigned to tag <script type="math/tex">t</script> can have non-zero probability. Essentially, we strictly enforce the constrain that a given word be given a single tag throughout a corpus.</p>

<p>When the model makes this decision it can use a type-level prior on how likely it is that a word is a determiner. Determiners, or articles, in general are very frequent at the token level (they occur a lot in sentences), but there are very few unique words which are determiners. Another thing we can do is have features on a word type in a <a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">naive-bayes</a> fashion. We assume that each word is a bag of feature-type and feature-value pairs which are generated from the tag assigned to the word. The features you might have on a word type are what is the suffix of the word? Is it capitalized? You can configure these features very easily.</p>

<p>Let’s summarize the model. Assume that the vocabulary of a language consists of <script type="math/tex">n</script> word types. The probability of a type-level tag assignment is given by:</p>

<script type="math/tex; mode=display">P(\mathbf{T},\mathbf{W}) = \prod_{i=1}^n P(T_i) P(W_i | T_i) = \prod_{i=1}^n P(T_i) \left( \prod_{(f,v) \in W_i} P(v | f, T_i) \right)</script>

<p>where, <script type="math/tex">(f,v)</script> is a feature-type and feature-value pair in the word type (e.g., <code>(:hasPunctuation, false)</code>.  So each tag <script type="math/tex">t</script> has a distribution over the values for each feature type. For instance, the common noun, tag 42 in our examples so far, is somewhat likely to have punctuation in the word (as in “roly-poly”). It’s distribution over the <code>:hasPunctuation</code> feature-type might look like:<sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup></p>

<script type="math/tex; mode=display">\left\{ false: 0.95, true: 0.05 \right\}</script>

<p>Once the tag assignments have been generated, everything proceeds identically to the standard token-level HMM except with the constraint that emission distributions have been constrained  so that a tag can only emit a word if that word has been assigned to the tag.</p>

<h3>How do you learn?</h3>
<p>The fairly simple change to the model made in the last section not only yields better performance, but also makes learning much simpler and efficient. Learning and inference will be done using <a href="http://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs Sampling</a>. I can’t go over Gibbs Sampling fully, but I’ll summarize the idea in the context of this work.  The random variable we don’t know in this model are the type-level assignments <script type="math/tex">\mathbf{T} = T_1,\ldots, T_n</script>. In the context of Bayesian models, we are interested in the posterior <script type="math/tex">P(\mathbf{T} | \mathbf{W}, \mathbf{w})</script>, where <script type="math/tex">\mathbf{W}</script> and <script type="math/tex">\mathbf{w}</script> denote the word types in the vocabulary and  the tokens of the corpus respectively; essentially, they’re both observed data.<sup id="fnref:observed"><a href="#fn:observed" class="footnote">4</a></sup> We can obtain samples from this posterior by repeatedly sampling each of the <script type="math/tex">T_i</script> variables with the  other assignments, denoted <script type="math/tex">\mathbf{T}_{-i}</script>, fixed. We sample <script type="math/tex">T_i</script>  according to the posterior <script type="math/tex">P(T_i | \mathbf{T}_{-i}, \mathbf{W}, \mathbf{w})</script>, which basically reprsents the following probability: If I assume all my other tag assignments are correct, what is the distribution for the tag assignment to the <script type="math/tex">i</script>th word. It’s relatively straightforward to show that if we continually update the sampling state <script type="math/tex">\mathbf{T}</script> one-tag-at-a-time in this way, at some point, the sampling state <script type="math/tex">\mathbf{T}</script> is drawn from the desired posterior <script type="math/tex">P(\mathbf{T} | \mathbf{W}, \mathbf{w})</script>.<sup id="fnref:gibbs"><a href="#fn:gibbs" class="footnote">5</a></sup> So essentially, learning boils down to looping over tagging assignments and sampling values while all other decisions are fixed.</p>

<p>In the original HMM, when using Gibbs Sampling, the state consists of all token-level assignments of words to tags. So the number of variables you need to sample is proportional to the number of words in the corpus, which can be massive. In this model, we only need to sample a variable for each word type, which is substantially smaller, and importantly grows very slowly relative to the amount of data you want to learn on.</p>

<p>Okay, so learning with this model boils down to how to compute the local posterior:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{array}{cl}
        P(T_i = t| \mathbf{T}_{-i}, \mathbf{W}, \mathbf{w})
             \propto& P(T_i = t | \mathbf{T}_{-i}) P(W_i | T_i = t,\mathbf{T}_{-i},\mathbf{W}_{-i}) \\
             & P(\mathbf{w} | T_i = t, \mathbf{T}_{-i})
\end{array} %]]></script>

<table>
  <tbody>
    <tr>
      <td>Let me break down each of these terms. The $P(T_i = t</td>
      <td>\mathbf{T}<em>{-i})$ is straight-forward to compute; if we count all the other tag assignments, the probability of assigning $T_i$ to $t$ is given by, $ \frac{n</em>{t} + \alpha}{n-1 + \alpha} $ where $n_t$ is the number of tags in $\mathbf{T}_{-i}$ which are currently assigned to $t$. The $\alpha$ term is the smoothing concentration parameter.<sup id="fnref:params"><a href="#fn:params" class="footnote">6</a></sup></td>
    </tr>
  </tbody>
</table>

<p>A similar reasoning is used to compute,</p>

<script type="math/tex; mode=display">P(W_i | T_i = t,\mathbf{T}_{-i}) = \prod_{(f,v) \in W_i} P(v | f, T_i = t, \mathbf{T}_{-i}, \mathbf{W}_{-i})</script>

<p>which decomposes a product over the various features on the word type. Each individual feature probability can be computed by using counts of how often a feature value is seen for other words assigned to the same tag.</p>

<p>The last term requires a little thinking. For the purpose of Gibbs Sampling, any probability term which doesn’t involve the thing we’re sampling, we can safely drop. At the token-level, the assignment of the <script type="math/tex">i</script>th word type to <script type="math/tex">t</script> only affects the local contexts in which the <script type="math/tex">i</script>th word type appears. Let’s use <script type="math/tex">w</script> to denote the <script type="math/tex">i</script>th word type. Each usage of <script type="math/tex">w</script> in the corpora are associated with a previous (before) word and a following (after) word.<sup id="fnref:pad"><a href="#fn:pad" class="footnote">7</a></sup> Let’s use <script type="math/tex">(b,w,a)</script> to represent the before word, the word itself, and the after word; so <script type="math/tex">(b,w,a)</script> represents a trigram in the corpus. Let <script type="math/tex">T(b)</script> and <script type="math/tex">T(a)</script> denote the tag assignments to words <script type="math/tex">b</script> and <script type="math/tex">a</script> (this is given to us by <script type="math/tex">\mathbf{T}</script>). The only probability terms associated with this usage which not constant with respect to the <script type="math/tex">T_i = t</script> assignment are:</p>

<script type="math/tex; mode=display">P(w | T_i = t, \mathbf{T}_{-i}, \mathbf{w}_{-i}) P(t | T(b), \mathbf{T}) P(T(a) | t, \mathbf{T})</script>

<p>These terms are the probability of the word itself with the considered tag, the probability of transitioning to tag <script type="math/tex">t</script> from the tag assigned to the previous word, and transitioning to the tag assigned to the successor word. The only terms which are relevant to the assignment come from all the context usages of the <script type="math/tex">i</script>th word type.
Specifically, if <script type="math/tex">C_i</script>  represents the multi-set of such context usages, we have <script type="math/tex">P(\mathbf{w} | T_i=t, \mathbf{T}_{-i})</script> is proportional to a product of the terms
in each <script type="math/tex">(b,w,a)</script> usage.  These probabilities can be computed by storing corpus level counts. Specifically for each word, we need counts of the <code>(before, after)</code> words as well as the counts for all individual words.</p>

<h2>Finally, walking through the implementation!</h2>
<p>Okay, so after a lot of prep work, we’re ready to dissect the code. I’m going to go linearly through the code and explain how each piece work. For reference, the full
script can be found <a href="http://gist.github.com/578348">here</a>.</p>

<h3>It's all about counters</h3>
<p>So one of the basic data abstractions you need for probabilistic computing is a counter.<sup id="fnref:library"><a href="#fn:library" class="footnote">8</a></sup> Essentially, a counter is a map of items to their counts, that needs, for computing probabilities, to support a fast way to get the sum of all counts. Here’s the code snippet that declares the appropriate data structure as well as the important methods. The proper way to do this is to make Counter a protocol (which I’ve done in my NLP clojure library <a href="http://github.com/aria42/mochi/blob/master/src/mochi/counter.clj">here</a>):</p>

<noscript><pre>400: Invalid request
</pre></noscript>
<script src="https://gist.github.com/587011.js"> </script>

<p>The two functions here are the only two we need for a counter: <code>inc-count</code> increments a count and returns a new counter, and <code>get-count</code> returns the current count. Since in Gibbs Sampling, none of our counts should be negative, we add an important <code>:post</code> check on <code>get-count</code> which will likely catch bugs.</p>

<h3>Dirichlet Distributions</h3>

<p>Once we have the <code>counter</code> abstraction, it’s very straightforward to build a probability distribution; all the distributions here are over a finite number of possible events. This kind of distribution is called a <a href="http://en.wikipedia.org/wiki/Multinomial_distribution">multinomial</a>. Here, we use a <code>DiricheltMultinomial</code> which represent the a multinomial drawn from the symmetric <a href="http://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet distribution</a>, which essentially means that all outcomes are given “fake” counts to smooth the probabilities (i.e., ensure no probability becomes zero or too small). The kinds of things we want to do with a distribution, simply include asking for the log-probability<sup id="fnref:underflow"><a href="#fn:underflow" class="footnote">9</a></sup> and making a weighted observation which changes the probabilities the distribution produces. Here’s the code. I’ll give more explanation and examples after:</p>

<noscript><pre>400: Invalid request
</pre></noscript>
<script src="https://gist.github.com/587021.js"> </script>

<p><b>Paragraph can be safely skipped</b>: The probabilities we need from the <code>DirichletMultinomial</code> are actually the “predictive” probabilities obtained from integrating out the Dirichelt parameters. Specifically, suppose  we have a distribution with <script type="math/tex">n</script> possible event outcomes and  assume the multinomial over these <script type="math/tex">n</script> events are drawn <script type="math/tex">\theta \sim Dirichlet(n, \alpha)</script>. Without observing any data, all <script type="math/tex">n</script> outcomes are equally likely. Now, suppose we have observed data <script type="math/tex">\mathbf{X}</script> and that <script type="math/tex">n_i</script> is the number of times, we have observed the <script type="math/tex">i</script>th outcome in <script type="math/tex">\mathbf{X}</script>. Then, we want the probability of a new event <script type="math/tex">e^*</script> given the observed data,</p>

<script type="math/tex; mode=display">P(e^* = i | \mathbf{X}) = \int_\theta P(\theta | \mathbf{X}) P(e^* = i | \theta) d \theta = \frac{n_i + \alpha}{\left(\sum_{i'} n_{i'}\right) + n * \alpha}</script>

<p>Given, a counter over events, we can efficiently compute a given probability. Each probability depends on knowing: the count of the event (<code>get-count</code>), the sum over all counts for all events (<code>total</code> from the <code>counter</code>), as well as the number of unique keys that this distribution could emit (<code>num-keys</code>). The reason we don’t just look at the number of keys in the counter is because we’re interested in the number of <emph>possible</emph> values; at any given time, we may not have counts for all possible events.</p>

<p>Making an observation to a distribution, in this context, just requires increment the count of the event so that subsequent calls to <code>log-prob</code> reflect this observation.</p>

<h3>What's in a word?</h3>
<p>Okay, now that we have some standard code out of the way, we need to do some POS-specific code. I’m going ot use a record <code>WordInfo</code> which represents all the information we need about a word in order to efficiently support Gibbs Sampling inference. This information includes: a string of the word itself, its total count in the corpus, a map of the feature-type and feature-value pairs, and a counter over the pairs of the context words which occur before and after word (specifically it will be a counter over <code>[before-word after-word]</code> pairs). Here’s the code:</p>

<noscript><pre>400: Invalid request
</pre></noscript>
<script src="https://gist.github.com/587038.js"> </script>

<p>The <code>get-feats</code> function simply returns a map of the feature-type (a keyword here) and its value. You can easily edit this function to have other features and the rest of the code will just work.</p>

<p>Now that we have this data-structure, we need to build this data structure to represent the statistics from a large corpus. Okay, suppose that I want to update the word-info for a given word after having observed a usage. The only info we need from the usage is the before and after word:</p>

<noscript><pre>400: Invalid request
</pre></noscript>
<script src="https://gist.github.com/587045.js"> </script>

<p>Two things need to change: (1) We need to increment the total usage of the word (the <code>:count</code> field in <code>WordInfo</code>). (2) We need to increment the count of the before-and-after pair <code>[before after]</code> in the counter for the context usages. Here’s what I love about clojure: If you design your abstractions and functions correctly, they work seamlessly with the language. If you don’t know the <code>-&gt;</code> threading macro: learn it, live it, love it. I think in conjunction with the <code>update-in</code> function, it allows for very succinct functions to update several piece of a complex data structure.</p>

<p>Okay, so let me show you the rest of the pieces which build of the word info data structures from a corpus (a seq of sentences):</p>

<noscript><pre>400: Invalid request
</pre></noscript>
<script src="https://gist.github.com/587049.js"> </script>

<p>What we want to do in <code>tally-sent</code> is update the word-info records for a given sentence. For this function, we have a map from a word string to its corresponding <code>WordInfo</code> record. The <code>(partition 3 1 sent)</code> produces a sequence of <code>(before-word word after-word)</code> trigrams which are all we need to update against. For each <code>word</code> in this triple, we ensure we have a <code>WordInfo</code> record (is there a <code>assoc-if-absent</code> function in core or contrib). And then we use our <code>tally-usage</code> function to update against the before and after word. Finally, we perform this update over all the sentences of a corpus in <code>build-vocab</code>.</p>

<h3>Gibbs Sampling State</h3>
<p>Let’s talk about how we represent the state of the Gibbs Sampler. Okay state is a dirty word in Clojure, and luckily the usage of state here is from Statistics and it represents an immutable value: for a given point in Gibbs Sampling, what are all the relevant assignments and the derived corpus counts from this assignment. Here’s the code:</p>

<noscript><pre>400: Invalid request
</pre></noscript>
<script src="https://gist.github.com/587057.js"> </script>

<p>I think the comments are sufficient here. The one thing that I should explain is that given the corpus and the <code>type-assigns</code>, all the other fields are determined and could theoretically be computing on the fly as needed. For efficiency however, it’s better to update those counts incrementally.</p>

<h3>Updating Gibbs Sampling State After an assignment/unassignment</h3>

<p>Now there are a lot of functions we need to write to support what happens when you add an assignment of a tag to a given word type or remove the assignment. These operations are the same, except when you make an assignment you are adding positive counts, and when you are unassigning, you remove counts. All these functions tend to take a <code>weight</code> to allow code reuse for these operations. Okay, so let’s take the case of updating the emission distribution associated with the tag which has been assigned/unassigned to a word-info. Two things need to change: we need to change the number of possible values the distribution can produce. If we are assigning the tag to the word, there is another possible outcome for the emission distribution; similarly we need to decrement if we are removing the assignment. Also, we need to observe the word the number of times it occurs in the corpus.</p>

<noscript><pre>400: Invalid request
</pre></noscript>
<script src="https://gist.github.com/587062.js"> </script>

<p>To be clear, <code>tag-emission-distr</code> is obtained from <code>(get-in state [:emission-distrs , tag])</code> where <code>state</code> is an instance of <code>State</code>.</p>

<p>There are analogous functions for updating the counts for the feature distributions and for the transitions. I’ll briefly go over updating the transitions since it’s bit trickier. When we assign a word to a tag, we need to loop over the <code>[before-word after-word]</code> counts in the <code>WordInfo</code> and, depending on the current tagging assignment, change these counts. Here’s the code:</p>

<noscript><pre>400: Invalid request
</pre></noscript>
<script src="https://gist.github.com/587068.js"> </script>

<h3>Gibbs Sampling Step</h3>
<p>Okay, so let’s take a top-down perspective for looking at how we make a simple Gibbs Sampling step. We first take our current state, unassign the current assignment to a word, and then sample a new value from the distribution <script type="math/tex">P(T_i = t| \mathbf{T}_{-i}, \mathbf{W}, \mathbf{w})</script>:</p>

<noscript><pre>400: Invalid request
</pre></noscript>
<script src="https://gist.github.com/587070.js"> </script>

<p>I didn’t show you the <code class="highlighter-rouge">assign</code> and <code class="highlighter-rouge">unassign</code> functions. All they do is update the Gibbs Sampling state data structures to reflect the change in assignment for a given word as discussed above. They both are nice pure functions and return new states.</p>

<p>You also haven’t seen <code>score-assign</code> and <code class="highlighter-rouge">sample-from-scores</code>, which I’ll discuss now. <code class="highlighter-rouge">score-assign</code> will return something proportional to the log-probability of
<script type="math/tex">P(T_i = t| \mathbf{T}_{-i}, \mathbf{W}, \mathbf{w})</script>. <code class="highlighter-rouge">sample-from-scores</code> will take these scores from the possible assignments and sample one.</p>

<p>Here’s <code>score-assign</code>:</p>
<noscript><pre>400: Invalid request
</pre></noscript>
<script src="https://gist.github.com/587075.js"> </script>

<p>The <code class="highlighter-rouge">(log-prob (:tag-prior state)  tag)</code> corresponds to <script type="math/tex">P(T_i = t | \mathbf{T}_{-i})</script>. The following <code>sum</code> form corresponds to the log of <script type="math/tex">\prod_{(f,v) \in W_i} P(v | f, T_i)</script>, the probability of the bundle of features associated with a given word type conditioned on the tag. The last top-level form (headed by <code>let</code>) has all the token-level terms:<br />
<script type="math/tex">P(w | \mathbf{T},\mathbf{w}_{-i})^{n_i} \prod_{(b,a) \in C_i}  P(t | T(b), \mathbf{T}) P(T(a) | t,\mathbf{T})</script>.
That <code>let</code> statement needs to suppose that the tag assignment has already happened to correctly compute the probability of the word under the tag. The inner <code>sum</code> term for each <code>[[before-word after-word] count]</code> entry adds the log-probabilities for all these usages (I also lump in the word log-probability itself, although this could be in a separate term weighted with the total occurrence of the word).</p>

<p>Note that the time it takes to score a given assignment is proportional to the number of unique contexts in which a word appears.</p>

<p>Once we have this function, we need to sample proportionally to these log-probabilities. Here is some very standard machine learning code that would normally be in a standard library:</p>

<noscript><pre>400: Invalid request
</pre></noscript>
<script src="https://gist.github.com/587090.js"> </script>

<h3>All the rest...</h3>
<p>From here, I think the rest of the code is straightforward. An iteration of the code consists of sampling each word’s assignment. There is a lot of code towards the end for initializing state. The complexity here is due to the fact that I need to initialize all maps with distributions with the correct number of possible keys. I hope this code make sense.</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>For each tag $t$, there are transition parameters $\psi_t$ over successor tags, drawn from a Dirichlet distribution over $K$ elements and hyper-parameter $\alpha$. These parameterize the transition distribution. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:distribution">
      <p>This distribution is parametrized from a symmetric Dirichelt with hyper-parameter <script type="math/tex">\beta</script> over <script type="math/tex">K</script> possible tags. <a href="#fnref:distribution" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>For each tag and feature-type, the distribution is parametrized by a symmetric Dirichlet over all possible feature-values and hyper-parameter <script type="math/tex">\beta</script>. <a href="#fnref:3" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:observed">
      <p>Note that the token-level tags <script type="math/tex">\mathbf{t}</script> are determined by type-assignments <script type="math/tex">\mathbf{T}</script>, since each word can only have one tag which can generate it. <a href="#fnref:observed" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:gibbs">
      <p>In practice, for any real problem, one doesn’t know when Gibbs Sampling, or MCMC in general, has “burned in”. <a href="#fnref:gibbs" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:params">
      <p>I don’t have the room to discuss this here, but this probability represents the “predictive” distribution obtained by integrating out the distribution parameters. <a href="#fnref:params" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:pad">
      <p>We pad each sentence with start and stop symbols to ensure this. <a href="#fnref:pad" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:library">
      <p>A lot of the names for these abstractions come from <a href="http://www.cs.berkeley.edu/~klein/">Dan Klein</a>, my PhD advisor, but I’m pretty sure modulo the name, the abstractions are pretty universal from my survey of machine learning libraries. <a href="#fnref:library" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:underflow">
      <p>To guard against numerical underflow, we work primarily with log-probabilities. <a href="#fnref:underflow" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

        </section>
    </div>
</article>


<footer class="site-footer">
        <div class="contact">
            <a href="http://twitter.com/aria42" class="home"><i class="ss-icon">twitter</i></a>
            <a href="http://github.com/aria42" class="home"><i class="ss-icon">octocat</i></a>
            <a href="http://linkedin.com/in/aria42" class="home"><i class="ss-icon">linkedin</i></a>
            <a href="mailto:me@aria42.com" class="home" style="margin-right:0em"><i class="ss-icon">email</i></a>
        </div>
        <div class="blah">
        &copy; 2016 <a href="http://aria42.com/">aria42.com</a>
        </div>
</footer>

<script type="text/javascript" src="//ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js"></script>





<script src="/webfonts/ss-social.js"></script>
<script>
  function toggleAbstract(elem) {
      $(elem).parents(".academic-paper").find(".abstract").toggle();
  }

  $(".full img").on("click", function() {
  $(this).toggleClass("zoom");
});

SVGElement.prototype.addClass = function (className) {
  if (!this.hasClass(className)) {
    this.setAttribute('class', this.getAttribute('class') + ' ' + className);
  }
};

$("a.nav-text-button").on({ 'touchstart' : function(){
  $(this).addClass('active');
}});

$("a.nav-text-button").on({ 'touchend' : function(){
  $(this).removeClass('active');
}});
$("a.nav-text-button").on({ 'touchleave' : function(){
  $(this).removeClass('active');
}});

function svgFill() {
  $('img[src$="svg"]').hide()
    .each(function(i, item) {
      var _this = this;
      return $.get(this.src).success(function(data) {
        var $svg, a, nName, nValue, _attr, _i, _len;
        $svg = $(data).find('svg');
        _attr = _this.attributes;
        $.extend(_attr, $svg[0].attributes);
        for (_i = 0, _len = _attr.length; _i < _len; _i++) {
          a = _attr[_i];
          nName = a.nodeName;
          nValue = a.nodeValue;
          if (nName !== 'src' && nName !== 'style') {
            $svg.attr(nName, nValue);
          }
        }
        return $(_this).replaceWith($svg);
      });
  });
}

var accented = "#428bca";

function setupHeader() {
  var uri = window.location.pathname.substring(1);
  if (uri.indexOf("/") >= 0) {
    uri = uri.substring(0, uri.indexOf("/"));
  }
  if (uri == "") {
    var loop = setInterval(function() {
      if ($(".nav-button-home svg ellipse").length) {
        $(".nav-button-home svg ellipse").attr("fill",accented);
      }
    }, 5)
  } else {
    $(".nav-button-" + uri).addClass("accented");
  }
}

(function() {
  $(svgFill);
  $(setupHeader);
}).call(this);


</script>

  <!--google analytics tracking code here-->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-57015155-1', 'auto');
  ga('send', 'pageview');

</script>
<!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=5151559;
var sc_invisible=1;
var sc_security="c3abe7d5";
</script>
<script type="text/javascript"
src="http://www.statcounter.com/counter/counter.js"></script>
<!-- End of StatCounter Code for Default Guide -->




</body>
</html>
